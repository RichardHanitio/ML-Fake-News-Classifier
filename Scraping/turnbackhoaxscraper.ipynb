{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for page request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content(url) :\n",
    "  page = requests.get(url)\n",
    "  soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "  soup_content = soup.find(\"div\", id = \"main-content\")\n",
    "  return soup_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class News:\n",
    "  def __init__(self, judul, narasi, tanggal, sumber, kategori, penulis) :\n",
    "    self.judul = judul\n",
    "    self.narasi = narasi\n",
    "    self.tanggal = tanggal\n",
    "    self.sumber = sumber\n",
    "    self.kategori = kategori\n",
    "    self.penulis = penulis\n",
    "\n",
    "  def __str__(self) :\n",
    "    return f\"\"\"\n",
    "    {{  \n",
    "      Judul : {self.judul},\\n\n",
    "      Narasi : {self.narasi},\\n\n",
    "      Tanggal : {self.tanggal},\\n\n",
    "      Sumber : {self.sumber}, \\n\n",
    "      Kategori : {self.kategori}, \\n\n",
    "      Penulis : {self.penulis}\\n\n",
    "    }}\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to replace \\<br> with \\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_breaks(element) :\n",
    "  for br in element.find_all(\"br\"):\n",
    "    br.replace_with(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to get each news' attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_attributes(link) :\n",
    "  # fetch link\n",
    "  get_url = requests.get(link)\n",
    "  news_soup = BeautifulSoup(get_url.text, \"html\")\n",
    "\n",
    "  # judul\n",
    "  judul = news_soup.find(\"h1\").text[8:]\n",
    "  \n",
    "  # narasi\n",
    "  news_body = news_soup.find(\"div\", class_=\"entry-content\").find_all(\"p\")\n",
    "  narasi_start_idx = 0\n",
    "  narasi = \"\"\n",
    "\n",
    "  for idx, paragraf in enumerate(news_body) :\n",
    "    # if \"narasi\" in paragraf.text.lower() :\n",
    "    if re.search(r\"=*\\[?narasi\\]?\\s*[:]?\", paragraf.text, re.I) :\n",
    "      narasi_start_idx = idx\n",
    "      break\n",
    "      \n",
    "  for idx, paragraf in enumerate(news_body[narasi_start_idx:]) :\n",
    "    if \"penjelasan\" in paragraf.text.lower() :\n",
    "      break\n",
    "    if paragraf : handle_breaks(paragraf)\n",
    "    \n",
    "    if idx==0 :\n",
    "      new_narasi = re.sub(r\"=*\\[?narasi\\]?\\s*[:]?\", \"\", paragraf.text, flags=re.I) \n",
    "      narasi += new_narasi\n",
    "    else :\n",
    "      narasi+=paragraf.text\n",
    "\n",
    "  # tanggal\n",
    "  tanggal = news_soup.find(\"span\", class_=\"entry-meta-date\").find(\"a\").decode_contents()\n",
    "\n",
    "  # sumber\n",
    "  sumber = \"\"\n",
    "  for idx, paragraf in enumerate(news_body) :\n",
    "    match = re.search(r\"=*\\[?sumber\\]?\\s*[:]?\", paragraf.text, re.I)\n",
    "    if match :\n",
    "      sumber = paragraf.text[match.end():].strip()\n",
    "      \n",
    "\n",
    "  # kategori\n",
    "  temp_kategori = news_soup.find(lambda tag: tag.name == \"p\" and \"kategori\" in tag.text.strip().lower())\n",
    "  if temp_kategori :\n",
    "    handle_breaks(temp_kategori)\n",
    "    new_kategori = re.sub(r\"=*\\[?kategori\\]?\\s*[:]?\", \"\", temp_kategori.text, flags=re.I) \n",
    "    kategori = new_kategori\n",
    "  else :\n",
    "    kategori = \"\"\n",
    "\n",
    "  # penulis\n",
    "  temp_penulis = news_soup.find(\"span\", class_=\"entry-meta-author\").find(\"a\", class_=\"fn\")\n",
    "  if temp_penulis :\n",
    "    penulis = temp_penulis.decode_contents()\n",
    "  else :\n",
    "    penulis = \"\"\n",
    "  \n",
    "  return News(judul, narasi, tanggal, sumber, kategori, penulis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to make a new dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe() :\n",
    "  columns = [\"judul\", \"narasi\", \"tanggal\", \"sumber\", \"kategori\", \"penulis\"]\n",
    "  df = pd.DataFrame(columns = columns)\n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to extract individual news' link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_individual_link(element) :\n",
    "  articles = element.find_all(\"article\")\n",
    "  links = []\n",
    "  for article in articles :\n",
    "    link = article.find(\"figure\", class_=\"mh-loop-thumb\").find(\"a\").get(\"href\")\n",
    "    links.append(link)\n",
    "  return links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to insert all news attributes to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_news(df, news) :\n",
    "  new_row = {\n",
    "    \"judul\" : news.judul,\n",
    "    \"narasi\" : news.narasi,\n",
    "    \"tanggal\" : news.tanggal,\n",
    "    \"sumber\" : news.sumber,\n",
    "    \"kategori\" : news.kategori,\n",
    "    \"penulis\" : news.penulis\n",
    "  }\n",
    "  df.loc[len(df)] = new_row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to download csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_csv(df) :\n",
    "  df.to_csv(r'C:/Users/Acer/Desktop/Skripsi/Code/program/turnbackhoax-2.csv', encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = create_dataframe()\n",
    "for page in range(12, 17) :\n",
    "  page_content = get_content(f\"https://turnbackhoax.id/page/{page}\")\n",
    "  news_links = extract_individual_link(page_content)\n",
    "  for news_link in news_links :\n",
    "    attr = get_news_attributes(news_link)\n",
    "    insert_news(df, attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_csv(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
