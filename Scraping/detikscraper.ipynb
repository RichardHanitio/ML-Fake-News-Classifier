{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "from urllib.parse import quote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for page request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content(url) :\n",
    "  page = requests.get(url)\n",
    "  soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "  soup_content = soup.find_all(\"div\", class_= \"container\")[3]\n",
    "  return soup_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class News:\n",
    "  def __init__(self, judul, narasi, tanggal, sumber, kategori, penulis) :\n",
    "    self.judul = judul\n",
    "    self.narasi = narasi\n",
    "    self.tanggal = tanggal\n",
    "    self.sumber = sumber\n",
    "    self.kategori = kategori\n",
    "    self.penulis = penulis\n",
    "  \n",
    "  def __str__(self) :\n",
    "    return f\"\"\"\n",
    "    {{  \n",
    "      Judul : {self.judul},\\n\n",
    "      Narasi : {self.narasi},\\n\n",
    "      Tanggal : {self.tanggal},\\n\n",
    "      Sumber : {self.sumber}, \\n\n",
    "      Kategori : {self.kategori}, \\n\n",
    "      Penulis : {self.penulis}\\n\n",
    "    }}\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to replace \\<br> with \\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_breaks(element) :\n",
    "  for br in element.find_all(\"br\"):\n",
    "    br.replace_with(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to get each news' attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_attributes(link) :\n",
    "  # fetch link\n",
    "  get_url = requests.get(link)\n",
    "  news_soup = BeautifulSoup(get_url.text, \"html\")\n",
    "\n",
    "  # judul\n",
    "  judul = news_soup.find(\"h1\", class_=\"detail__title\")\n",
    "  if judul :\n",
    "    judul = judul.text.strip()\n",
    "  else :\n",
    "    judul = \"\"\n",
    "  \n",
    "  # narasi\n",
    "  [strong.decompose() for strong in news_soup.find_all(\"strong\")]\n",
    "  [div_paradetail.decompose() for div_paradetail in news_soup.find_all(\"div\", class_=\"paradetail\")]\n",
    "  [div_lihatjg.decompose() for div_lihatjg in news_soup.find_all(\"div\", class_=\"lihatjg\")]\n",
    "  [div_nav.decompose() for div_nav in news_soup.find_all(\"div\", class_=\"nav\")]\n",
    "  [div_detail.decompose() for div_detail in news_soup.find_all(\"div\", class_=\"detail__multiple\")]\n",
    "  [table_linksisip.decompose() for table_linksisip in news_soup.find_all(\"table\", class_=\"linksisip\")]\n",
    "  [video.decompose() for video in news_soup.find_all(\"video\")]\n",
    "  [ads.decompose() for ads in news_soup.find_all(\"div\", class_=\"parallaxindetail scrollpage\" )]\n",
    "  [pemiluads.decompose() for pemiluads in news_soup.find_all(\"div\", class_=\"cb-pemilu\")]\n",
    "\n",
    "  narasi = news_soup.find(\"div\", class_=\"detail__body-text\")\n",
    "  if narasi :\n",
    "    narasi = narasi.text.strip()\n",
    "  else :\n",
    "    narasi = \"\"\n",
    "\n",
    "  # tanggal\n",
    "  tanggal = news_soup.find(\"div\", class_=\"detail__date\")\n",
    "  if tanggal :\n",
    "    tanggal = tanggal.text.strip()\n",
    "  else : tanggal = \"\"\n",
    "\n",
    "  # sumber\n",
    "  sumber = \"detik\" \n",
    "\n",
    "  # kategori\n",
    "  kategori = news_soup.find(\"span\", class_=\"detail__label\")\n",
    "  if kategori :\n",
    "    kategori = kategori.text.strip()\n",
    "  else : kategori = \"\"\n",
    "\n",
    "  # penulis\n",
    "  penulis_span = news_soup.find(\"span\", class_=\"detail__label\")\n",
    "  if penulis_span : penulis_span.decompose()\n",
    "  penulis = news_soup.find(\"div\", class_=\"detail__author\")\n",
    "  if penulis :\n",
    "    penulis = penulis.text\n",
    "  else : penulis = \"\"\n",
    "  \n",
    "  return News(judul, narasi, tanggal, sumber, kategori, penulis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to extract individual news' link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_individual_link(element) :\n",
    "  articles = element.find_all(\"article\", class_=\"list-content__item\")\n",
    "  links = []\n",
    "  for article in articles :\n",
    "    link = article.find(\"a\", class_=\"media__link\").get(\"href\")\n",
    "    links.append(link)\n",
    "  return links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to make a new dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe() :\n",
    "  columns = [\"judul\", \"narasi\", \"tanggal\", \"sumber\", \"kategori\", \"penulis\"]\n",
    "  df = pd.DataFrame(columns = columns)\n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to insert all news attributes to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_news(df, news) :\n",
    "  new_row = {\n",
    "    \"judul\" : news.judul,\n",
    "    \"narasi\" : news.narasi,\n",
    "    \"tanggal\" : news.tanggal,\n",
    "    \"sumber\" : news.sumber,\n",
    "    \"kategori\" : news.kategori,\n",
    "    \"penulis\" : news.penulis\n",
    "  }\n",
    "  df.loc[len(df)] = new_row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to create list of dates starting from start_date to end_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_date_list(start_date, end_date) :\n",
    "  return [quote(date.strftime(\"%m/%d/%Y\"), safe=\"\") for date in pd.date_range(start = start_date, end = end_date)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to download CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_csv(df) :\n",
    "  df.to_csv(r'C:/Users/Acer/Desktop/Skripsi/Code/program/detikcom-6.csv', encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = create_dataframe()\n",
    "for date in create_date_list(\"8/22/2023\",\"8/31/2023\"):\n",
    "  page_content = get_content(f\"https://news.detik.com/indeks?date={date}\")\n",
    "  news_links = extract_individual_link(page_content)\n",
    "  for news_link in news_links :\n",
    "    attr = get_news_attributes(news_link)\n",
    "    insert_news(df, attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_csv(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
